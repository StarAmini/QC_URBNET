##QC_URBNET_src_QcFunction2

tm_gross_error <- function(xts_vector) {
  
  # Identify dates with outliers (values > 60 or < -40)
  row_mask <- apply(xts_vector, 1, function(x) any(x > 60 | x < -40, na.rm = TRUE))

  rm_dates <- index(xts_vector)[row_mask]
  
  # Create a copy of the input data for cleaning and flagging
  qc_data <- qc_data_flagged <- xts_vector
  
  # Set outlier values to NA in qc_data
  if(length(rm_dates) > 0) {
    
    qc_data[rm_dates] <- NA
    
  }
  
  # Flag outliers in qc_data_flagged
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0  # Default flag to 0

  if(length(rm_dates) > 0) {
    
    qc_data_flagged[rm_dates] <- 1  # Flag outliers as 1

  }
  
  # Create the output list
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
}

#######################################################################
season_thresholds <- list(
  winter = list(min_val = -24.44, max_val = 23.54),
  spring = list(min_val = -21.05, max_val = 38.08),
  summer = list(min_val = 7.85, max_val = 46.26),
  autumn = list(min_val = -8.42, max_val = 40.87)
)


# Helper function to determine the season of a given date
get_season <- function(date) {
  month <- as.numeric(format(date, "%m"))
  if (month %in% c(12, 1, 2)) {
    return("winter")
  } else if (month %in% 3:5) {
    return("spring")
  } else if (month %in% 6:8) {
    return("summer")
  } else {
    return("autumn")
  }
}

# Updated function to handle seasonal thresholds
tm_range_outliers_seasonal <- function(xts_vector, season_thresholds) {
  # Get dates and initialize outputs
  dates <- index(xts_vector)
  seasons <- sapply(dates, get_season)  # Determine season for each date
  
  qc_data <- xts_vector  # Initialize cleaned data
  qc_data_flagged <- xts_vector  # Initialize flagged data
  
  qc_data_flagged[] <- 0  # Default: no flags
  
  for (season in names(season_thresholds)) {
    # Retrieve seasonal thresholds
    thresholds <- season_thresholds[[season]]
    season_mask <- seasons == season  # Identify dates in the current season
    
    # Identify indices outside threshold range for this season
    outlier_indices <- which(xts_vector < thresholds$min_val | 
                         xts_vector > thresholds$max_val)

    # Apply the season mask
     rm_dates <- outlier_indices[seasons[outlier_indices] == season]
    
    # Set flagged and cleaned data for the outliers
    qc_data[rm_dates] <- NA
    qc_data_flagged[rm_dates] <- 1
  }
  
  # Output the results as a list
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
}

###########################################################################################

tm_temporal_outliers <- function(xts_vector, dt = 3, diff = 6) {
  if (!inherits(xts_vector, "xts")) stop("Input must be an xts object")
  
  qc_data <- xts_vector
  qc_data_flagged <- xts_vector
  qc_data_flagged[] <- 0
  
  for (station in colnames(xts_vector)) {
    X <- as.numeric(xts_vector[, station])
    n <- length(X)
    PO <- rep(FALSE, n)
    
    for (i in 1:n) {
      if (is.na(X[i])) next
      neighbors <- max(1, i - dt):min(n, i + dt)
      if (sum(!is.na(X[neighbors])) < (1 + dt)) next
      
      med_val <- median(X[neighbors], na.rm = TRUE)
      med_diff <- abs(X[i] - med_val)
      if (med_diff <= diff) next
      
      left_diff <- if (!is.na(X[i - 1])) abs(X[i] - X[i - 1]) else NA
      right_diff <- if (!is.na(X[i + 1])) abs(X[i] - X[i + 1]) else NA
      
      # If both sides exist, both must exceed diff
      if (!is.na(left_diff) && !is.na(right_diff)) {
        if (left_diff > diff && right_diff > diff) PO[i] <- TRUE
      } else {
        # If only one side exists, flag if median difference exceeds diff
        PO[i] <- TRUE
      }
    }
    
    qc_data_flagged[PO, station] <- 1
    qc_data[PO, station] <- NA
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}

###########################################################################################################
tm_persistence_outliers3_main <- function(xts_vector, window_size = "3 hours", by = "10 min", na_tolerance = 5, min_non_na = 1) {
  
  # Ensure time series is not empty
  if (ncol(xts_vector) == 0) stop("No stations detected in the dataset.")
  
  # Determine the interval between observations (assumes regular time series)
  time_interval <- as.numeric(difftime(index(xts_vector)[2], index(xts_vector)[1], units = "mins"))
  if (is.na(time_interval) || time_interval == 0) stop("Time index not recognized correctly.")
  
  # Compute number of intervals in a 3-hour window
  width <- as.integer(180 / time_interval) + 1  # 180 minutes / interval length in minutes
  
  # Initialize output xts objects
  qc_data <- xts_vector
  qc_data_flagged <- xts(matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)), order.by = index(xts_vector))
  colnames(qc_data_flagged) <- colnames(xts_vector)
  # Custom SD function with NA tolerance
  custom_sd <- function(x, na_tolerance = 5, min_non_na = 1) {
  # na_tolerance is now the max number of NAs allowed in the window
  na_count <- sum(is.na(x))
  
  # skip if too many NAs
  if (na_count > na_tolerance) {
    return(NA_real_)
  }
  
  # Otherwise, require at least some non-NA data to compute SD
  if (sum(!is.na(x)) < min_non_na) {
    return(NA_real_)
  }
  
  sd(x, na.rm = TRUE)
}
  
  # Loop through each station (column)
  for (station in colnames(xts_vector)) {
    # Compute rolling standard deviation
    rolling_sd <- rollapply(
  data  = xts_vector[, station, drop = FALSE],
  width = width,
  FUN   = function(z) custom_sd(z, na_tolerance = 5, min_non_na = 5),
  fill  = NA,
  align = "right"
)

    threshold <- 0.001 
    # Identify timestamps where SD is near zero
    flagged_timestamps <- index(xts_vector)[which(rolling_sd < threshold )]
   
    if (length(flagged_timestamps) > 0) {
      # Expand flagged timestamps to include the full rolling window
      expanded_dates <- unique(do.call("c", lapply(flagged_timestamps, function(ts) {
        seq(from = ts - (width - 1) * time_interval * 60, by = by, length.out = width)
      })))
      
      # Ensure flagged dates are within the dataset index
      expanded_dates <- expanded_dates[expanded_dates %in% index(xts_vector)]
      
      # Flag data
      qc_data[expanded_dates, station] <- NA
      qc_data_flagged[expanded_dates, station] <- 1
    }
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}

#################################################################################################
tm_persistence_outliers3_main2 <- function(xts_vector,
                                          window_size = "5 hours",
                                          by = "10 min",
                                          na_tolerance = 5,
                                          min_non_na = 1) {
  
  # Ensure time series is not empty
  if (ncol(xts_vector) == 0) stop("No stations detected in the dataset.")
  
  # Determine the interval between observations (assumes regular time series)
  time_interval <- as.numeric(difftime(index(xts_vector)[2],
                                       index(xts_vector)[1],
                                       units = "mins"))
  if (is.na(time_interval) || time_interval == 0) {
    stop("Time index not recognized correctly.")
  }
  
  # Helper: convert window_size to minutes
  window_to_minutes <- function(x) {
    if (is.numeric(x)) {
      return(as.numeric(x))  # assume minutes
    }
    if (inherits(x, "difftime")) {
      return(as.numeric(x, units = "mins"))
    }
    if (is.character(x)) {
      parts <- strsplit(trimws(x), "\\s+")[[1]]
      if (length(parts) < 1) {
        stop("window_size must be like '5 hours', '180 mins', numeric minutes, or difftime.")
      }
      
      value <- suppressWarnings(as.numeric(parts[1]))
      if (is.na(value)) {
        stop("First part of window_size must be numeric, e.g. '5 hours'.")
      }
      
      unit <- if (length(parts) >= 2) tolower(parts[2]) else "mins"
      mult <- switch(
        unit,
        "min" = 1, "mins" = 1, "minute" = 1, "minutes" = 1,
        "hour" = 60, "hours" = 60,
        "day" = 1440, "days" = 1440,
        stop("Unsupported unit in window_size (use minutes, hours, or days).")
      )
      return(value * mult)
    }
    
    stop("Unsupported type for window_size.")
  }
  
  # Compute number of intervals in the chosen window
  window_minutes <- window_to_minutes(window_size)
  width <- as.integer(window_minutes / time_interval) + 1
  
  # Initialize output xts objects
  qc_data <- xts_vector
  qc_data_flagged <- xts(
    matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)),
    order.by = index(xts_vector)
  )
  colnames(qc_data_flagged) <- colnames(xts_vector)
  
  # Function: is the window constant (all non-NA values equal)?
  constant_window <- function(x, na_tolerance, min_non_na) {
    na_count <- sum(is.na(x))
    
    # too many NAs, skip
    if (na_count > na_tolerance) {
      return(NA_real_)
    }
    
    x_non <- x[!is.na(x)]
    
    # not enough data
    if (length(x_non) < min_non_na) {
      return(NA_real_)
    }
    
    # all values equal -> return 1, else 0
    if (length(x_non) == 0) {
      return(NA_real_)
    }
    
    is_const <- (max(x_non) - min(x_non)) == 0
    as.numeric(is_const)  # 1 or 0
  }
  
  # Loop through each station (column)
  for (station in colnames(xts_vector)) {
    # Rolling indicator: 1 if window is constant (dT/dt = 0), 0 otherwise
    rolling_const <- rollapply(
      data  = xts_vector[, station, drop = FALSE],
      width = width,
      FUN   = function(z) constant_window(z,
                                          na_tolerance = na_tolerance,
                                          min_non_na   = min_non_na),
      fill  = NA,
      align = "right"
    )
    
    # Timestamps where the window is constant (all differences = 0)
    flagged_timestamps <- index(xts_vector)[which(rolling_const == 1)]
    
    if (length(flagged_timestamps) > 0) {
      # Expand flagged timestamps to include the full rolling window
      expanded_dates <- unique(do.call("c", lapply(flagged_timestamps, function(ts) {
        seq(
          from       = ts - (width - 1) * time_interval * 60,
          by         = by,
          length.out = width
        )
      })))
      
      # Ensure flagged dates are within the dataset index
      expanded_dates <- expanded_dates[expanded_dates %in% index(xts_vector)]
      
      # Flag data
      qc_data[expanded_dates, station]         <- NA
      qc_data_flagged[expanded_dates, station] <- 1
    }
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}


###########################################################################################################33

tm_extreme_quantiles_outliers <- function(xts_vector,
                                          ext_lim_factor = 4){
  
  tavg_filter_l <- sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    quantile(xts_obj_sample, .25, na.rm = TRUE) - ext_lim_factor * IQR(xts_obj_sample, na.rm = TRUE)
  })
  
  tavg_filter_h <- sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    quantile(xts_obj_sample, .75, na.rm = TRUE) + ext_lim_factor * IQR(xts_obj_sample, na.rm = TRUE)
  })
  
  rm_dates <- do.call(c, sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    zoo::index(
      xts_obj_sample[xts_obj_sample <= tavg_filter_l[x] |
                       xts_obj_sample >= tavg_filter_h[x]]
    )
  }))
  
  qc_data <- qc_data_flagged <- xts_vector
  qc_data[rm_dates] <- NA
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0
  qc_data_flagged[rm_dates] <- 1
  
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
  
}

#####################################################################################################

get_spatial_nearby_points <- function(xy_target,
                                      xyz_metadata,
                                      lmt_xy,
                                      lmt_n)
{
  
  pos_target <- match(xy_target, xyz_metadata$ID)
  
  target <- xyz_metadata[pos_target, c("LON", "LAT")]
  nearby <- xyz_metadata[, c("LON", "LAT")]
  
  out <- xyz_metadata[, c("ID", "LON", "LAT")]
  out$distance <- geosphere::distHaversine(target, nearby)
  out <- out[out$distance < lmt_xy, ]
  out <- out[order(out$distance), ]
  out <- out[1:(lmt_n + 1), ]
  out <- out[complete.cases(out), ]
  out <- as.character(out$ID)
  
  return(out)
  
}


spatemp_value_check_NOLU2 <- function(xy_target,
                                      xy_database,
                                      xts_database,
                                      params = list(
                                        lmt_xy_ = 2500,
                                        lmt_n_ = 5,
                                        n_nearby_ = 5)
) {
  
  # spatial check
  
  xy_nearby <- get_spatial_nearby_points(xy_target = xy_target,
                                         xyz_metadata = xy_database,
                                         lmt_xy = params$lmt_xy_,
                                         lmt_n = params$lmt_n_)
  xy_nearby <- xy_nearby[-1]
  
  target_xts <- xts_database[, xy_target]
  nearby_xts <- xts_database[, xy_nearby]
  
  dif_target_nearby <- lapply(
    seq_len(ncol(nearby_xts)),
    function(idd) {
      
      dif_ts <- target_xts - nearby_xts[, idd]
      
      dif_098 <- quantile(dif_ts, 0.9888, na.rm = TRUE)
      flag_098_pos <- time(dif_ts[dif_ts >= dif_098])
      
      dif_1_098 <- quantile(dif_ts, 1 - 0.9888, na.rm = TRUE)
      flag_098_neg <- time(dif_ts[dif_ts <= dif_1_098])
      
      out_ts <- dif_ts
      out_ts[!is.na(out_ts)] <- 0
      out_ts[c(flag_098_pos, flag_098_neg)] <- 1
      
      out_ts
    }
  )
  
  dif_target_nearby <- do.call(cbind, dif_target_nearby)
  dif_target_nearby <- rowSums(dif_target_nearby, na.rm = TRUE)
  # time step in which at least 5 nearby points are flagged
  dif_target_nearby <- which(dif_target_nearby >= params$n_nearby_)
  # getting the dates
  dif_target_nearby <- time(target_xts)[dif_target_nearby]
  dif_target_nearby <- as.character(dif_target_nearby)
  
  # temporal check
  # previous date
  dif_target_target_pre <-  target_xts - stats::lag(target_xts, k = -1)
  quant_098_pre <- quantile(
    dif_target_target_pre,
    0.9888,
    na.rm = TRUE
  )
  dif_target_target_pre <- dif_target_target_pre[
    dif_target_target_pre > quant_098_pre |
      dif_target_target_pre < -quant_098_pre
  ]
  dif_target_target_pre <- as.character(time(dif_target_target_pre))
  
  # next date
  dif_target_target_nex <-  stats::lag(target_xts, k = 1) - target_xts
  quant_098_next <- quantile(
    dif_target_target_nex[dif_target_target_nex > 0],
    0.9888,
    na.rm = TRUE
  )
  dif_target_target_nex <- dif_target_target_nex[
    dif_target_target_nex > quant_098_next |
      dif_target_target_nex < -quant_098_next
  ]
  dif_target_target_nex <- as.character(time(dif_target_target_nex))
  dif_target_target <- intersect(dif_target_target_pre, dif_target_target_nex)
  
  # intersecting both checks
  rm_dates <- intersect(dif_target_target, dif_target_nearby)
  
  
  print("rm_dates:")
  print(rm_dates)
  print(paste("Number of dates in rm_dates: ", length(rm_dates)))
  
  qc_data <- qc_data_flagged <- target_xts
  qc_data[rm_dates] <- NA
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0
  qc_data_flagged[rm_dates] <- 1
  
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged,
    rm_dates = rm_dates # Add rm_dates here
  )
  
  return(out)
}
##########################################################################

spatial_outlier_detection <- function(xy_database, xts_database, params = list(
  threshold_multiplier = 3.0,
  weight_decay = NULL
)) {
compute_distance_matrix <- function() {
  sensor_ids <- xy_database$ID
  n <- length(sensor_ids)
  dist_matrix <- matrix(0, nrow = n, ncol = n, dimnames = list(sensor_ids, sensor_ids))
  
  for (i in seq_len(n)) {
    lat1 <- xy_database$LAT[i]
    lon1 <- xy_database$LON[i]
    for (j in seq_len(n)) {
      lat2 <- xy_database$LAT[j]
      lon2 <- xy_database$LON[j]
      dist_matrix[i, j] <- distHaversine(c(lon1, lat1), c(lon2, lat2))
    }
  }
  
  return(as.data.frame(dist_matrix))
}

spatial_outlier_detection <- function(params = list(
  threshold_multiplier = 3.0,
  weight_decay = NULL
)) {
  
  dist_matrix <- compute_distance_matrix()
  diag(dist_matrix) <- NA
  
  if (is.null(params$weight_decay)) {
    params$weight_decay <- median(as.numeric(dist_matrix), na.rm = TRUE)
  }
  
  weight_matrix <- exp(- (as.matrix(dist_matrix)^2) / (2 * params$weight_decay^2))
  weight_matrix[is.na(weight_matrix)] <- 0
  
  sensor_ids <- intersect(colnames(xts_database), xy_database$ID)
  ts_data <- xts_database[, sensor_ids]
  qc_data <- ts_data
  qc_data_flagged <- ts_data
  qc_data_flagged[] <- 0
  
  ts_arr <- as.matrix(ts_data)
  time_index <- index(xts_database)
  n_time <- nrow(ts_arr)
  n_sensor <- ncol(ts_arr)
  
  for (t in seq_len(n_time)) {
    x <- ts_arr[t, ]
    valid_mask <- !is.na(x)
    x_filled <- ifelse(is.na(x), 0, x)
    
    weight_sum <- rowSums(weight_matrix * valid_mask, na.rm = TRUE)
    weighted_sum <- weight_matrix %*% x_filled
    weighted_avg <- weighted_sum / weight_sum
    
    diff <- sweep(weight_matrix * valid_mask, 2, weighted_avg, "-")
    weighted_var <- rowSums((diff^2) * valid_mask, na.rm = TRUE) / weight_sum
    weighted_std <- sqrt(weighted_var)
    
    outlier_mask <- abs(x - weighted_avg) > params$threshold_multiplier * weighted_std & !is.na(x)
    
    qc_data[t, ] <- ifelse(outlier_mask, NA, x)
    qc_data_flagged[t, ] <- as.integer(outlier_mask)
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}
}




