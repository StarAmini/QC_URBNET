##QC_URBNET_src_QcFunction2

tm_gross_error <- function(xts_vector) {
  
  # Identify dates with outliers (values > 60 or < -40)
  rm_dates <- time(xts_vector[xts_vector > 60 | xts_vector < -40 | xts_vector == -40 | xts_vector == 60])
  
  # Create a copy of the input data for cleaning and flagging
  qc_data <- qc_data_flagged <- xts_vector
  
  # Set outlier values to NA in qc_data
  qc_data[rm_dates] <- NA
  
  # Flag outliers in qc_data_flagged
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0  # Default flag to 0
  qc_data_flagged[rm_dates] <- 1  # Flag outliers as 1
  
  # Create the output list
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
}

#######################################################################
season_thresholds <- list(
  winter = list(min_val = -24.44, max_val = 23.54),
  spring = list(min_val = -21.05, max_val = 38.08),
  summer = list(min_val = 7.85, max_val = 46.26),
  autumn = list(min_val = -8.42, max_val = 40.87)
)


# Helper function to determine the season of a given date
get_season <- function(date) {
  month <- as.numeric(format(date, "%m"))
  if (month %in% c(12, 1, 2)) {
    return("winter")
  } else if (month %in% 3:5) {
    return("spring")
  } else if (month %in% 6:8) {
    return("summer")
  } else {
    return("autumn")
  }
}

# Updated function to handle seasonal thresholds
tm_range_outliers_seasonal <- function(xts_vector, season_thresholds) {
  # Get dates and initialize outputs
  dates <- index(xts_vector)
  seasons <- sapply(dates, get_season)  # Determine season for each date
  
  qc_data <- xts_vector  # Initialize cleaned data
  qc_data_flagged <- xts_vector  # Initialize flagged data
  
  qc_data_flagged[] <- 0  # Default: no flags
  
  for (season in names(season_thresholds)) {
    # Retrieve seasonal thresholds
    thresholds <- season_thresholds[[season]]
    season_mask <- seasons == season  # Identify dates in the current season
    
    # Identify indices outside threshold range for this season
    outlier_indices <- which(xts_vector < thresholds$min_val | 
                               xts_vector > thresholds$max_val)
    
    # Apply the season mask
    rm_dates<- outlier_indices[season_mask[outlier_indices]]
    
    # Set flagged and cleaned data for the outliers
    qc_data[rm_dates] <- NA
    qc_data_flagged[rm_dates] <- 1
  }
  
  # Output the results as a list
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
}

###########################################################################################

tm_temporal_outliers <- function(xts_vector, dt = 3, diff = 6) {
  # Ensure the input is an xts object
  if (!inherits(xts_vector, "xts")) {
    stop("Input must be an xts object")
  }
  
  time_index <- index(xts_vector)  # Extract time index
  station_names <- colnames(xts_vector)  # Get column names (stations)
  
  # Create output objects
  qc_data <- xts_vector
  qc_data_flagged <- xts_vector
  qc_data_flagged[] <- 0  # Initialize all flags to 0
  
  # Loop through each station (column)
  for (station in station_names) {
    X <- as.numeric(xts_vector[, station])  # Convert to numeric
    n <- length(X)
    PO <- rep(FALSE, n)  # Boolean vector for outliers
    
    # Identify outliers based on neighboring median
    for (i in 1:n) {
      neighbors <- max(1, i - dt):min(n, i + dt)
      if (!is.na(X[i]) && sum(!is.na(X[neighbors])) >= (1 + dt)) {
        PO[i] <- abs(X[i] - median(X[neighbors], na.rm = TRUE)) >= diff
      }
    }
    
    # Update flagged data
    qc_data_flagged[PO, station] <- 1  # Mark outliers
    qc_data[PO, station] <- NA  # Replace outliers with NA
  }
  
  # Return a list similar to tm_temporal_outliers
  return(list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  ))
}

######################################################################################

tm_persistence_outliers0 <- function(xts_vector, min_change = 0.04, n_window = 0.5, by = "10 min", length_out = 18) {
  # Calculate absolute differences
  diffs <- abs(diff(xts_vector))
  
  # Custom sum function to handle NA tolerance
  custom_sum <- function(xts_vector_chunk, n_window = 0.5) {
    n_nas <- sum(is.na(xts_vector_chunk))  # Count the number of NAs
    if (n_nas > n_window * length(xts_vector_chunk)) {
      return(NA)  # Too many NAs, return NA
    } else {
      return(sum(xts_vector_chunk, na.rm = TRUE))  # Sum non-NA values
    }
  }
  
  # Apply rolling window sum
  rolling_sum <- zoo::rollapply(diffs, width = 18, FUN = function(chunk) custom_sum(chunk, n_window = n_window), fill = NA, align = "right")
  
  # Flag bins where total change < min_change
  flagged_periods <- rolling_sum < min_change
  flagged_timestamps <- which(flagged_periods)
  
  # Generate flagged date ranges
  flagged_dates <- lapply(flagged_timestamps, function(ts) {
    seq(from = index(xts_vector)[ts], by = by, length.out = length_out)
  })
  
  # Concatenate flagged dates into a single vector
  if (length(flagged_dates) == 0) {
    rm_dates <- NULL
  } else {
    rm_dates <- unique(do.call("c", flagged_dates))
  }
  
  # Ensure flagged dates are within the series index
  rm_dates <- rm_dates[rm_dates %in% index(xts_vector)]
  
  # Build the output
  qc_data <- xts_vector
  qc_data[rm_dates] <- NA
  
  # Create binary flag series
  qc_data_flagged <- xts(rep(0, length(index(xts_vector))), order.by = index(xts_vector))
  qc_data_flagged[rm_dates] <- 1
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}


tm_persistence_outliers1 <- function(xts_vector, n_window = 0.5, by = "10 min", length_out = 18) {
  # Custom function to calculate standard deviation with NA tolerance
  custom_sd <- function(chunk, n_window = 0.5) {
    n_nas <- sum(is.na(chunk))  # Count number of NAs
    if (n_nas > n_window * length(chunk)) {
      return(NA)  # Too many NAs, return NA
    } else {
      return(sd(chunk, na.rm = TRUE))  # Calculate standard deviation, ignoring NAs
    }
  }
  
  # Apply rolling window of 3 hours (18 intervals for 10-minute data)
  rolling_sd <- zoo::rollapply(
    xts_vector,
    width = 18,  # 18 intervals for 10-minute frequency (equivalent to 3 hours)
    FUN = function(chunk) custom_sd(chunk, n_window = n_window),
    fill = NA,
    align = "center"  # Assign results at the right end of the window
  )
  
  # Flag windows with standard deviation == 0
  flagged_periods <- rolling_sd == 0
  flagged_timestamps <- which(flagged_periods)
  
  # Generate flagged date ranges (covering the window of length `length_out`)
  flagged_dates <- lapply(flagged_timestamps, function(ts) {
    seq(from = index(xts_vector)[ts], by = by, length.out = length_out)
  })
  
  # Concatenate flagged dates into a single vector
  if (length(flagged_dates) == 0) {
    rm_dates <- NULL
  } else {
    rm_dates <- unique(do.call("c", flagged_dates))
  }
  
  # Ensure flagged dates are within the index of the original series
  rm_dates <- rm_dates[rm_dates %in% index(xts_vector)]
  
  # Create flagged data
  qc_data <- xts_vector
  qc_data[rm_dates] <- NA
  
  # Create binary flag series (1 for flagged, 0 otherwise)
  qc_data_flagged <- xts(rep(0, length(index(xts_vector))), order.by = index(xts_vector))
  qc_data_flagged[rm_dates] <- 1
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}


tm_persistence_outliers2 <- function(xts_vector, window_size = "3 hours", by = "10 min", na_tolerance = 0.5, min_non_na = 5) {
  
  # Ensure time series is not empty
  if (ncol(xts_vector) == 0) stop("No stations detected in the dataset.")
  
  # Determine the interval between observations (assumes regular time series)
  time_interval <- as.numeric(difftime(index(xts_vector)[2], index(xts_vector)[1], units = "mins"))
  if (is.na(time_interval) || time_interval == 0) stop("Time index not recognized correctly.")
  
  # Compute number of intervals in a 3-hour window
  width <- as.integer(180 / time_interval)  # 180 minutes / interval length in minutes
  
  # Initialize output xts objects
  qc_data <- xts_vector
  qc_data_flagged <- xts(matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)), order.by = index(xts_vector))
  colnames(qc_data_flagged) <- colnames(xts_vector)

  # Custom SD function with NA tolerance
  custom_sd <- function(x, na_tolerance = 0.5, min_non_na = 5) {
  total_len <- length(x)
  n_valid <- sum(!is.na(x))
  frac_na <- 1 - (n_valid / total_len)
  
  if (n_valid > min_non_na || frac_na > na_tolerance) {
    return(NA)  # Skip window if too many NAs or too little data
  } else {
    return(sd(x, na.rm = TRUE))
  }
}
  
  # Loop through each station (column)
  for (station in colnames(xts_vector)) {
    # Compute rolling standard deviation
    rolling_sd <- rollapply(
  data = xts_vector[, station, drop = FALSE], 
  width = width, 
  FUN = function(x) custom_sd(x, na_tolerance = na_tolerance, min_non_na = min_non_na), 
  fill = NA, 
  align = "right"
)

    threshold <- 0.1 
    # Identify timestamps where SD is near zero
    flagged_timestamps <- index(xts_vector)[which(rolling_sd < threshold )]
    
    if (length(flagged_timestamps) > 0) {
      # Expand flagged timestamps to include the full rolling window
      expanded_dates <- unique(do.call("c", lapply(flagged_timestamps, function(ts) {
        seq(from = ts - (width - 1) * time_interval * 60, by = by, length.out = width)
      })))
      
      # Ensure flagged dates are within the dataset index
      expanded_dates <- expanded_dates[expanded_dates %in% index(xts_vector)]
      
      # Flag data
      qc_data[expanded_dates, station] <- NA
      qc_data_flagged[expanded_dates, station] <- 1
    }
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}

###########################################################################################################

tm_persistence_outliers4 <- function(xts_vector,
                                     window_hours = 3,
                                     na_tolerance = 0.5,   # now a proportion of the window
                                     min_non_na   = 1,     # still at least one non-NA
                                     threshold    = 0.01) {
  # 1. sanity checks
  if (ncol(xts_vector) == 0) 
    stop("No stations detected in the dataset.")
  if (nrow(xts_vector) < 2) 
    stop("Time series too short to compute intervals.")
  
  # 2. infer sampling interval in minutes
  dt_mins <- as.numeric(difftime(index(xts_vector)[2],
                                 index(xts_vector)[1],
                                 units = "mins"))
  if (is.na(dt_mins) || dt_mins <= 0) 
    stop("Time index not recognized correctly.")
  
  # 3. compute window width in rows
  window_secs <- window_hours * 3600
  width       <- floor(window_secs / (dt_mins * 60)) + 1
  
  # 4. prepare outputs
  qc_data         <- xts_vector
  qc_data_flagged <- xts(
    matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)),
    order.by = index(xts_vector)
  )
  colnames(qc_data_flagged) <- colnames(xts_vector)
  
  # 5. custom SD allowing up to na_tolerance fraction of NAs
  custom_sd <- function(x) {
    total <- length(x)
    n_na  <- sum(is.na(x))
    if (n_na / total > na_tolerance)      # too many NAs
      return(NA_real_)
    if (sum(!is.na(x)) < min_non_na)      # not enough real data
      return(NA_real_)
    sd(x, na.rm = TRUE)
  }
  
  # 6. loop each station
  for (station in colnames(xts_vector)) {
    rolling_sd <- rollapply(
      xts_vector[, station, drop = FALSE],
      width = width,
      FUN   = custom_sd,
      fill  = NA,
      align = "right"
    )
    
    bad_times <- index(xts_vector)[which(rolling_sd < threshold)]
    if (length(bad_times) == 0) next
    
    # build mask for all rows within 3h before any bad_time
    times   <- index(xts_vector)
    to_flag <- rep(FALSE, length(times))
    for (ts in bad_times) {
      window_start <- ts - window_secs
      to_flag <- to_flag | (times >= window_start & times <= ts)
    }
    
    qc_data[to_flag, station]         <- NA
    qc_data_flagged[to_flag, station] <- 1
  }
  
  # 7. return exactly what you asked for
  return(list(
    qc_data         = qc_data,
    qc_data_flagged = qc_data_flagged
  ))
}
################################################
tm_persistence_outliers3 <- function(
  xts_vector,
  window_size   = "3 hours",
  by            = "10 mins",
  na_tolerance  = 5,
  min_non_na    = 1,
  threshold     = 0.01
) {
  # Preconditions
  if (ncol(xts_vector) == 0) stop("No stations detected in the dataset.")
  idx <- index(xts_vector)
  if (length(idx) < 2) stop("Need at least two time points to determine interval.")

  # Determine sampling interval
  time_interval_secs <- as.numeric(difftime(idx[2], idx[1], units = "secs"))
  if (is.na(time_interval_secs) || time_interval_secs <= 0) stop("Time index not recognized correctly.")

  # Parse desired window duration
  window_dur <- as.duration(window_size)
  if (is.na(window_dur) || window_dur <= 0) stop("Invalid window_size.")
  # Compute number of points needed (ceiling ensures >= duration)
  width_pts <- ceiling(as.numeric(window_dur, units = "secs") / time_interval_secs)

  # Prepare output
  qc_data <- xts_vector
  qc_data_flagged <- xts(matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)),
                         order.by = idx)
  colnames(qc_data_flagged) <- colnames(xts_vector)

  # Helper: compute SD with NA tolerance, skip all-zero windows
  custom_sd <- function(z) {
    na_count <- sum(is.na(z))
    if (na_count > na_tolerance) return(NA_real_)
    nz <- z[!is.na(z)]
    if (length(nz) < min_non_na) return(NA_real_)
    # skip windows entirely zero
    if (all(nz == 0)) return(NA_real_)
    sd(nz)
  }

  # Iterate per station
  for (station in colnames(xts_vector)) {
    x <- xts_vector[, station, drop = FALSE]
    # rolling SD
    rolling_sd <- rollapply(coredata(x),
                             width = width_pts,
                             FUN = custom_sd,
                             fill = NA,
                             align = "right")

    # Indices where SD below threshold, ensure full window
    flagged_idx <- which(!is.na(rolling_sd) & rolling_sd < threshold)
    flagged_idx <- flagged_idx[flagged_idx >= width_pts]
    if (length(flagged_idx) == 0) next

    # Expand each flagged index to full window indices
    all_idx <- unique(unlist(lapply(flagged_idx, function(i) seq(i - width_pts + 1, i))))

    # Convert to timestamps then apply
    ts_to_flag <- idx[all_idx]
    qc_data[ts_to_flag, station] <- NA
    qc_data_flagged[ts_to_flag, station] <- 1
  }

  list(qc_data = qc_data, qc_data_flagged = qc_data_flagged)
}

############################################################################################

tm_persistence_outliers3_2 <- function(
  xts_vector,
  window_size   = "3 hours",
  by            = "10 mins",
  na_tolerance  = 5,
  min_non_na    = 1,
  threshold     = 0.01
) {
  if (ncol(xts_vector) == 0) stop("No stations detected in the dataset.")
  idx <- index(xts_vector)
  if (length(idx) < 2) stop("Need at least two time points to determine interval.")

  # Parse window_size into total seconds
  parse_window_secs <- function(ws) {
    if (inherits(ws, c("Duration", "Period"))) return(as.numeric(as.duration(ws), units = "secs"))
    if (is.numeric(ws)) return(ws)
    if (is.character(ws)) {
      m <- regexec(
        "^\\s*([0-9]+\\.?[0-9]*)\\s*(hours?|hrs?|h|minutes?|mins?|m|seconds?|secs?|s)\\s*$",
        ws,
        ignore.case = TRUE
      )
      reg <- regmatches(ws, m)[[1]]
      if (length(reg) == 3) {
        val <- as.numeric(reg[2])
        unit <- tolower(reg[3])
        if (startsWith(unit, "h")) return(val * 3600)
        if (startsWith(unit, "m")) return(val * 60)
        return(val)
      }
    }
    stop("Invalid window_size format; use numeric seconds or strings like '3 hours'.")
  }

  window_secs <- parse_window_secs(window_size)
  # Ensure at least 3-hour minimum
  min_secs <- 3 * 3600
  if (window_secs < min_secs) {
    warning("window_size less than 3 hours; enforcing minimum of 3 hours.")
    window_secs <- min_secs
  }

  # Determine sampling interval via median of diffs
  diffs_secs <- as.numeric(diff(idx), units = "secs")
  time_interval_secs <- median(diffs_secs, na.rm = TRUE)
  if (is.na(time_interval_secs) || time_interval_secs <= 0) stop("Time index not recognized correctly.")

  # Compute number of points in the window
  width_pts <- ceiling(window_secs / time_interval_secs)

  # Prepare outputs
  qc_data <- xts_vector
  qc_flag <- xts(matrix(0, nrow = nrow(xts_vector), ncol = ncol(xts_vector)), order.by = idx)
  colnames(qc_flag) <- colnames(xts_vector)

  # Custom SD skipping all-zero windows
  custom_sd <- function(z) {
    if (sum(is.na(z)) > na_tolerance) return(NA_real_)
    nz <- z[!is.na(z)]
    if (length(nz) < min_non_na) return(NA_real_)
    if (all(nz == 0)) return(NA_real_)
    sd(nz)
  }

  # Loop stations
  for (station in colnames(xts_vector)) {
    x <- coredata(xts_vector[, station, drop = FALSE])
    sd_roll <- rollapply(
      x,
      width = width_pts,
      FUN = custom_sd,
      fill = NA,
      align = "right"
    )

    flags <- which(!is.na(sd_roll) & sd_roll < threshold & seq_along(sd_roll) >= width_pts)
    if (!length(flags)) next

    all_idx <- unique(unlist(lapply(flags, function(i) seq(i - width_pts + 1, i))))
    ts_to_flag <- idx[all_idx]

    qc_data[ts_to_flag, station] <- NA
    qc_flag[ts_to_flag, station] <- 1
  }

  list(qc_data = qc_data, qc_data_flagged = qc_flag)
}

###########################################################################################################33

tm_extreme_quantiles_outliers <- function(xts_vector,
                                          ext_lim_factor = 4){
  
  tavg_filter_l <- sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    quantile(xts_obj_sample, .25, na.rm = TRUE) - ext_lim_factor * IQR(xts_obj_sample, na.rm = TRUE)
  })
  
  tavg_filter_h <- sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    quantile(xts_obj_sample, .75, na.rm = TRUE) + ext_lim_factor * IQR(xts_obj_sample, na.rm = TRUE)
  })
  
  rm_dates <- do.call(c, sapply(1:12, function(x) {
    xts_obj_sample <- xts_vector[, 1][as.numeric(format(time(xts_vector), "%m")) == x]
    zoo::index(
      xts_obj_sample[xts_obj_sample <= tavg_filter_l[x] |
                       xts_obj_sample >= tavg_filter_h[x]]
    )
  }))
  
  qc_data <- qc_data_flagged <- xts_vector
  qc_data[rm_dates] <- NA
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0
  qc_data_flagged[rm_dates] <- 1
  
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged
  )
  
  return(out)
  
}

#####################################################################################################

get_spatial_nearby_points <- function(xy_target,
                                      xyz_metadata,
                                      lmt_xy,
                                      lmt_n)
{
  
  pos_target <- match(xy_target, xyz_metadata$ID)
  
  target <- xyz_metadata[pos_target, c("LON", "LAT")]
  nearby <- xyz_metadata[, c("LON", "LAT")]
  
  out <- xyz_metadata[, c("ID", "LON", "LAT")]
  out$distance <- geosphere::distHaversine(target, nearby)
  out <- out[out$distance < lmt_xy, ]
  out <- out[order(out$distance), ]
  out <- out[1:(lmt_n + 1), ]
  out <- out[complete.cases(out), ]
  out <- as.character(out$ID)
  
  return(out)
  
}


spatemp_value_check_NOLU2 <- function(xy_target,
                                      xy_database,
                                      xts_database,
                                      params = list(
                                        lmt_xy_ = 2500,
                                        lmt_n_ = 5,
                                        n_nearby_ = 5)
) {
  
  # spatial check
  
  xy_nearby <- get_spatial_nearby_points(xy_target = xy_target,
                                         xyz_metadata = xy_database,
                                         lmt_xy = params$lmt_xy_,
                                         lmt_n = params$lmt_n_)
  xy_nearby <- xy_nearby[-1]
  
  target_xts <- xts_database[, xy_target]
  nearby_xts <- xts_database[, xy_nearby]
  
  dif_target_nearby <- lapply(
    seq_len(ncol(nearby_xts)),
    function(idd) {
      
      dif_ts <- target_xts - nearby_xts[, idd]
      
      dif_098 <- quantile(dif_ts, 0.9888, na.rm = TRUE)
      flag_098_pos <- time(dif_ts[dif_ts >= dif_098])
      
      dif_1_098 <- quantile(dif_ts, 1 - 0.9888, na.rm = TRUE)
      flag_098_neg <- time(dif_ts[dif_ts <= dif_1_098])
      
      out_ts <- dif_ts
      out_ts[!is.na(out_ts)] <- 0
      out_ts[c(flag_098_pos, flag_098_neg)] <- 1
      
      out_ts
    }
  )
  
  dif_target_nearby <- do.call(cbind, dif_target_nearby)
  dif_target_nearby <- rowSums(dif_target_nearby, na.rm = TRUE)
  # time step in which at least 5 nearby points are flagged
  dif_target_nearby <- which(dif_target_nearby >= params$n_nearby_)
  # getting the dates
  dif_target_nearby <- time(target_xts)[dif_target_nearby]
  dif_target_nearby <- as.character(dif_target_nearby)
  
  # temporal check
  # previous date
  dif_target_target_pre <-  target_xts - stats::lag(target_xts, k = -1)
  quant_098_pre <- quantile(
    dif_target_target_pre,
    0.9888,
    na.rm = TRUE
  )
  dif_target_target_pre <- dif_target_target_pre[
    dif_target_target_pre > quant_098_pre |
      dif_target_target_pre < -quant_098_pre
  ]
  dif_target_target_pre <- as.character(time(dif_target_target_pre))
  
  # next date
  dif_target_target_nex <-  stats::lag(target_xts, k = 1) - target_xts
  quant_098_next <- quantile(
    dif_target_target_nex[dif_target_target_nex > 0],
    0.9888,
    na.rm = TRUE
  )
  dif_target_target_nex <- dif_target_target_nex[
    dif_target_target_nex > quant_098_next |
      dif_target_target_nex < -quant_098_next
  ]
  dif_target_target_nex <- as.character(time(dif_target_target_nex))
  dif_target_target <- intersect(dif_target_target_pre, dif_target_target_nex)
  
  # intersecting both checks
  rm_dates <- intersect(dif_target_target, dif_target_nearby)
  
  
  print("rm_dates:")
  print(rm_dates)
  print(paste("Number of dates in rm_dates: ", length(rm_dates)))
  
  qc_data <- qc_data_flagged <- target_xts
  qc_data[rm_dates] <- NA
  qc_data_flagged[!is.na(qc_data_flagged)] <- 0
  qc_data_flagged[rm_dates] <- 1
  
  out <- list(
    qc_data = qc_data,
    qc_data_flagged = qc_data_flagged,
    rm_dates = rm_dates # Add rm_dates here
  )
  
  return(out)
}
##########################################################################

spatial_outlier_detection <- function(xy_database, xts_database, params = list(
  threshold_multiplier = 3.0,
  weight_decay = NULL
)) {
compute_distance_matrix <- function() {
  sensor_ids <- xy_database$ID
  n <- length(sensor_ids)
  dist_matrix <- matrix(0, nrow = n, ncol = n, dimnames = list(sensor_ids, sensor_ids))
  
  for (i in seq_len(n)) {
    lat1 <- xy_database$LAT[i]
    lon1 <- xy_database$LON[i]
    for (j in seq_len(n)) {
      lat2 <- xy_database$LAT[j]
      lon2 <- xy_database$LON[j]
      dist_matrix[i, j] <- distHaversine(c(lon1, lat1), c(lon2, lat2))
    }
  }
  
  return(as.data.frame(dist_matrix))
}

spatial_outlier_detection <- function(params = list(
  threshold_multiplier = 3.0,
  weight_decay = NULL
)) {
  
  dist_matrix <- compute_distance_matrix()
  diag(dist_matrix) <- NA
  
  if (is.null(params$weight_decay)) {
    params$weight_decay <- median(as.numeric(dist_matrix), na.rm = TRUE)
  }
  
  weight_matrix <- exp(- (as.matrix(dist_matrix)^2) / (2 * params$weight_decay^2))
  weight_matrix[is.na(weight_matrix)] <- 0
  
  sensor_ids <- intersect(colnames(xts_database), xy_database$ID)
  ts_data <- xts_database[, sensor_ids]
  qc_data <- ts_data
  qc_data_flagged <- ts_data
  qc_data_flagged[] <- 0
  
  ts_arr <- as.matrix(ts_data)
  time_index <- index(xts_database)
  n_time <- nrow(ts_arr)
  n_sensor <- ncol(ts_arr)
  
  for (t in seq_len(n_time)) {
    x <- ts_arr[t, ]
    valid_mask <- !is.na(x)
    x_filled <- ifelse(is.na(x), 0, x)
    
    weight_sum <- rowSums(weight_matrix * valid_mask, na.rm = TRUE)
    weighted_sum <- weight_matrix %*% x_filled
    weighted_avg <- weighted_sum / weight_sum
    
    diff <- sweep(weight_matrix * valid_mask, 2, weighted_avg, "-")
    weighted_var <- rowSums((diff^2) * valid_mask, na.rm = TRUE) / weight_sum
    weighted_std <- sqrt(weighted_var)
    
    outlier_mask <- abs(x - weighted_avg) > params$threshold_multiplier * weighted_std & !is.na(x)
    
    qc_data[t, ] <- ifelse(outlier_mask, NA, x)
    qc_data_flagged[t, ] <- as.integer(outlier_mask)
  }
  
  return(list(qc_data = qc_data, qc_data_flagged = qc_data_flagged))
}
}




